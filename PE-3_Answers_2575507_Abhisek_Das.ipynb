{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99edee20",
   "metadata": {},
   "source": [
    "# Fraud Detection with Logistic Regression and Feature Engineering\n",
    "You are a data scientist at a financial institution, and your primary task is to develop a fraud detection model using logistic regression. The dataset you have is highly imbalanced, with only a small fraction of transactions being fraudulent. Your objective is to create an effective model by implementing logistic regression and employing various feature engineering techniques to improve the model's performance:\n",
    "\n",
    "1. Data Preparation:\n",
    "    a. Load the dataset, and provide an overview of the available features, including transaction details, customer information, and labels (fraudulent or non-fraudulent).\n",
    "    b. Describe the class distribution of fraudulent and non-fraudulent transactions and discuss the imbalance issue.\n",
    "\n",
    "2. Initial Logistic Regression Model:\n",
    "    a. Implement a basic logistic regression model using the raw dataset.\n",
    "    b. Evaluate the model's performance using standard metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "3. Feature Engineering:\n",
    "    a. Apply feature engineering techniques to enhance the predictive power of the model. These techniques may include:\n",
    "    -Creating new features.\n",
    "    - Scaling or normalizing features.\n",
    "    - Handling missing values.\n",
    "    - Encoding categorical variables.\n",
    "    b. Explain why each feature engineering technique is relevant for fraud detection.\n",
    "\n",
    "4. Handling Imbalanced Data:\n",
    "    a. Discuss the challenges associated with imbalanced datasets in the context of fraud detection.\n",
    "    b. Implement strategies to address class imbalance, such as:\n",
    "    - Oversampling the minority class.\n",
    "    - Undersampling the majority class.\n",
    "    -Using synthetic data generation techniques (e.g., SMOTE).\n",
    "\n",
    "5. Logistic Regression with Feature-Engineered Data:\n",
    "    a. Train a logistic regression model using the feature-engineered dataset and the methods for handling imbalanced data.\n",
    "    b. Evaluate the model's performance using appropriate evaluation metrics.\n",
    "\n",
    "6. Model Interpretation:\n",
    "    a. Interpret the coefficients of the logistic regression model and discuss which features have the most influence on fraud detection.\n",
    "    b. Explain how the logistic regression model can be used for decision-making in identifying potential fraud.\n",
    "\n",
    "7. Model Comparison:\n",
    "    a. Compare the performance of the initial logistic regression model with the feature-engineered and balanced data model.\n",
    "    b. Discuss the advantages and limitations of each approach.\n",
    "\n",
    "8. Presentation and Recommendations:\n",
    "    a. Prepare a presentation or report summarizing your analysis, results, and recommendations for the financial institution. Highlight the importance of feature engineering and handling imbalanced data in building an effective fraud detection system.\n",
    "\n",
    "    In this case study, you are required to showcase your ability to preprocess data, implement logistic regression, apply feature engineering techniques, and address class imbalance to improve the model's performance. Your analysis should also demonstrate your understanding of the nuances of fraud detection in a financial context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3efdeb",
   "metadata": {},
   "source": [
    "1. Data Preparation:\n",
    "    a. Load the dataset, and provide an overview of the available features, including transaction details, customer information, and labels (fraudulent or non-fraudulent).\n",
    "    b. Describe the class distribution of fraudulent and non-fraudulent transactions and discuss the imbalance issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c24fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Load the dataset, and provide an overview of the available features, including transaction details, customer information, and labels (fraudulent or non-fraudulent).\n",
    "\n",
    "    step: represents a unit of time where 1 step equals 1 hour\n",
    "    type: type of online transaction\n",
    "    amount: the amount of the transaction\n",
    "    nameOrig: customer starting the transaction\n",
    "    oldbalanceOrg: balance before the transaction\n",
    "    newbalanceOrig: balance after the transaction\n",
    "    nameDest: recipient of the transaction\n",
    "    oldbalanceDest: initial balance of recipient before the transaction\n",
    "    newbalanceDest: the new balance of recipient after the transaction\n",
    "    isFraud: fraud transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e7132f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      "0    997\n",
      "1      9\n",
      "Name: isFraud, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#  b. Describe the class distribution of fraudulent and non-fraudulent transactions and discuss the imbalance issue.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "data = pd.read_csv('financialfraud.csv')\n",
    "\n",
    "# Describe the class distribution\n",
    "class_distribution = data['isFraud'].value_counts()\n",
    "\n",
    "# Print the class distribution\n",
    "print(\"Class Distribution:\")\n",
    "print(class_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f3304",
   "metadata": {},
   "source": [
    "# 2. Initial Logistic Regression Model:\n",
    "    a. Implement a basic logistic regression model using the raw dataset.\n",
    "    b. Evaluate the model's performance using standard metrics like accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8c518ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a. Implement a basic logistic regression model using the raw dataset.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Drop non-numeric columns and columns not being used for analysis\n",
    "data_cleaned = data.drop(columns=['nameOrig', 'nameDest'])\n",
    "\n",
    "# Perform one-hot encoding on the 'type' column\n",
    "data_encoded = pd.get_dummies(data_cleaned, columns=['type'])\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into features (X) and target variable (y) after encoding\n",
    "X = data_encoded.drop(columns=['isFraud'])\n",
    "y = data_encoded['isFraud']\n",
    "\n",
    "# Handle class imbalance by oversampling the minority class\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=2)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split the resampled data into training and testing sets\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=2)\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "logistic_model = LogisticRegression(random_state=2)\n",
    "logistic_model.fit(xtrain, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3b68643d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Metrics:\n",
      "Accuracy: 0.87\n",
      "Precision: 0.95\n",
      "Recall: 0.79\n",
      "F1-Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "# b. Evaluate the model's performance using standard metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Predictions on the test set\n",
    "ypred = logistic_model.predict(xtest)\n",
    "\n",
    "# Evaluate the model's performance using standard metrics\n",
    "accuracy = accuracy_score(ytest, ypred)\n",
    "precision = precision_score(ytest, ypred)\n",
    "recall = recall_score(ytest, ypred)\n",
    "f1 = f1_score(ytest, ypred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e0fc2",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering:\n",
    "    a. Apply feature engineering techniques to enhance the predictive power of the model. These techniques may include:\n",
    "        -Creating new features.\n",
    "        - Scaling or normalizing features.\n",
    "        - Handling missing values.\n",
    "        - Encoding categorical variables.\n",
    "    b. Explain why each feature engineering technique is relevant for fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fd514e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Metrics after Feature Engineering:\n",
      "Accuracy: 1.00\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1-Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhisek Das\\AppData\\Local\\Temp\\ipykernel_11608\\455214009.py:27: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  data_cleaned.fillna(data_cleaned.mean(), inplace=True)\n",
      "C:\\Users\\Abhisek Das\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Abhisek Das\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Abhisek Das\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "# a. Apply feature engineering techniques to enhance the predictive power of the model. These techniques may include:\n",
    "#       - Creating new features.\n",
    "#       - Scaling or normalizing features.\n",
    "#       - Handling missing values.\n",
    "#       - Encoding categorical variables.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "data = pd.read_csv('financialfraud.csv')\n",
    "\n",
    "# Drop non-numeric columns and columns not being used for analysis\n",
    "data_cleaned = data.drop(columns=['nameOrig', 'nameDest'])\n",
    "\n",
    "# Perform feature engineering: Creating new features\n",
    "data_cleaned['amount_ratio'] = data_cleaned['oldbalanceOrg'] / data_cleaned['amount']\n",
    "\n",
    "# Perform feature engineering: Scaling numerical features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_cleaned[['oldbalanceOrg', 'amount', 'amount_ratio']] = scaler.fit_transform(data_cleaned[['oldbalanceOrg', 'amount', 'amount_ratio']])\n",
    "\n",
    "# Perform feature engineering: Handling missing values (filling with mean)\n",
    "data_cleaned.fillna(data_cleaned.mean(), inplace=True)\n",
    "\n",
    "# Perform feature engineering: Encoding categorical variables (One-hot encoding 'type' column)\n",
    "data_encoded = pd.get_dummies(data_cleaned, columns=['type'])\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data_encoded.drop(columns=['isFraud'])\n",
    "y = data_encoded['isFraud']\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "logistic_model.fit(xtrain, ytrain)\n",
    "\n",
    "# Predictions on the test set\n",
    "ypred = logistic_model.predict(xtest)\n",
    "\n",
    "# Evaluate the model's performance using standard metrics\n",
    "accuracy = accuracy_score(ytest, ypred)\n",
    "precision = precision_score(ytest, ypred)\n",
    "recall = recall_score(ytest, ypred)\n",
    "f1 = f1_score(ytest, ypred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Model Performance Metrics after Feature Engineering:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47058634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Explain why each feature engineering technique is relevant for fraud detection.\n",
    "1. Creating New Features:\n",
    "    Purpose: Creating new features allows the model to capture more complex patterns in the data, which might not be evident in the original features. For example, creating ratios or aggregating information can reveal valuable insights.\n",
    "    Relevance for Fraud Detection: Creating features like transaction amount ratios (e.g., amount_ratio = oldbalanceOrg / amount) can highlight unusual transaction patterns. Fraudsters might exploit specific ratios to conduct fraudulent transactions, making these features relevant for fraud detection.\n",
    "\n",
    "2. Scaling or Normalizing Features:\n",
    "    Purpose: Scaling ensures that all features have a similar scale, preventing certain features from dominating the learning process. Normalization brings all features to a standard scale.\n",
    "    Relevance for Fraud Detection: In fraud detection, the magnitudes of features like transaction amounts and balances can vary significantly. Scaling ensures that no single feature unduly influences the model due to its scale, allowing the model to learn patterns more accurately.\n",
    "\n",
    "3. Handling Missing Values:\n",
    "    Purpose: Missing values can cause issues during model training. Addressing them is crucial to avoid biased model predictions.\n",
    "    Relevance for Fraud Detection: Incomplete or missing transaction data can occur for various reasons. Filling missing values, especially with methods like mean imputation, ensures that the model can utilize all available data, making predictions more reliable.\n",
    "\n",
    "4. Encoding Categorical Variables:\n",
    "    Purpose: Machine learning algorithms require numerical input, making it necessary to convert categorical variables into numerical representations.\n",
    "    Relevance for Fraud Detection: Variables like transaction types ('type') are categorical. One-hot encoding these variables ensures that the model can understand and analyze the different transaction types effectively. Fraudulent activities might be associated with specific transaction types, making their proper representation vital for fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668103ec",
   "metadata": {},
   "source": [
    "# 4. Handling Imbalanced Data:\n",
    "    a. Discuss the challenges associated with imbalanced datasets in the context of fraud detection.\n",
    "    b. Implement strategies to address class imbalance, such as:\n",
    "        - Oversampling the minority class.\n",
    "        - Undersampling the majority class.\n",
    "        - Using synthetic data generation techniques (e.g., SMOTE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d658c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Discuss the challenges associated with imbalanced datasets in the context of fraud detection.\n",
    "\n",
    "In fraud detection, dealing with imbalanced datasets poses several challenges:\n",
    "\n",
    "    Bias in Model Training: Models tend to be biased towards the majority class (non-fraudulent transactions) because they have more samples to learn from. As a result, the model may struggle to identify patterns related to the minority class (fraudulent transactions).\n",
    "\n",
    "    Inaccurate Evaluation: Traditional accuracy is not a reliable metric for imbalanced datasets. A model predicting all instances as non-fraudulent can achieve high accuracy but fail to identify any fraudulent transactions. Evaluation metrics like precision, recall, and F1-score are more informative but can still be affected by the class imbalance.\n",
    "\n",
    "    Loss of Critical Information: In undersampling, removing majority class samples can lead to loss of valuable information, potentially ignoring genuine non-fraudulent patterns. Oversampling, on the other hand, can lead to overfitting if not done carefully.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b70fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Implement strategies to address class imbalance, such as:\n",
    "    - Oversampling the minority class.\n",
    "    - Undersampling the majority class.\n",
    "    - Using synthetic data generation techniques (e.g., SMOTE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "442162c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after SMOTE and random undersampling: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhisek Das\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with the actual file name)\n",
    "data = pd.read_csv('financialfraud.csv')\n",
    "\n",
    "# Encode categorical variable 'type' using one-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(data[['type']])\n",
    "\n",
    "# Concatenate the encoded features with other numeric features\n",
    "X = pd.concat([pd.DataFrame(encoded_data.toarray(), columns=encoder.get_feature_names(['type'])), \n",
    "               data[['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig',\n",
    "                     'oldbalanceDest', 'newbalanceDest', 'isFlaggedFraud']]], axis=1)\n",
    "# Target variable\n",
    "y = data['isFraud']\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# Apply SMOTE for oversampling the minority class\n",
    "smote = SMOTE(random_state=2)\n",
    "X_resampled, y_resampled = smote.fit_resample(xtrain, ytrain)\n",
    "\n",
    "# Apply random undersampling for the majority class\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X_resampled, y_resampled)\n",
    "\n",
    "# Initialize and train the logistic regression model on resampled data\n",
    "logistic_model_resampled = LogisticRegression(random_state=2)\n",
    "logistic_model_resampled.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predictions on the test set\n",
    "ypred_resampled = logistic_model_resampled.predict(xtest)\n",
    "\n",
    "# Evaluate the model's performance using accuracy\n",
    "accuracy_resampled = accuracy_score(ytest, ypred_resampled)\n",
    "\n",
    "# Print the evaluation metric after resampling\n",
    "print(f\"Accuracy after SMOTE and random undersampling: {accuracy_resampled:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92988d8",
   "metadata": {},
   "source": [
    "# 5. Logistic Regression with Feature-Engineered Data:\n",
    "    a. Train a logistic regression model using the feature-engineered dataset and the methods for handling imbalanced data.\n",
    "    b. Evaluate the model's performance using appropriate evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "14ccb55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression Model using Feature-Engineered Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhisek Das\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# a. Train a logistic regression model using the feature-engineered dataset and the methods for handling imbalanced data.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = pd.read_csv('financialfraud.csv')\n",
    "\n",
    "# Encode categorical variable 'type' using one-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(data[['type']])\n",
    "\n",
    "# Concatenate the encoded features with other numeric features\n",
    "X = pd.concat([pd.DataFrame(encoded_data.toarray(), columns=encoder.get_feature_names(['type'])), \n",
    "               data[['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig',\n",
    "                     'oldbalanceDest', 'newbalanceDest', 'isFlaggedFraud']]], axis=1)\n",
    "# Target variable\n",
    "y = data['isFraud']\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "print(\"Training Logistic Regression Model using Feature-Engineered Data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b3f09eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the Model's Performance after Feature Engineering and Resampling...\n",
      "Accuracy after Feature Engineering and Resampling: 0.98\n"
     ]
    }
   ],
   "source": [
    "# b. Evaluate the model's performance using appropriate evaluation metrics.\n",
    "\n",
    "print(\"Evaluating the Model's Performance after Feature Engineering and Resampling...\")\n",
    "\n",
    "# Apply SMOTE for oversampling the minority class\n",
    "smote = SMOTE(random_state=2)\n",
    "X_resampled, y_resampled = smote.fit_resample(xtrain, ytrain)\n",
    "\n",
    "# Apply random undersampling for the majority class\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X_resampled, y_resampled)\n",
    "\n",
    "# Initialize and train the logistic regression model on resampled data\n",
    "logistic_model_resampled = LogisticRegression(random_state=2)\n",
    "logistic_model_resampled.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predictions on the test set\n",
    "ypred_resampled = logistic_model_resampled.predict(xtest)\n",
    "\n",
    "# Evaluate the model's performance using appropriate metrics\n",
    "accuracy_resampled = accuracy_score(ytest, ypred_resampled)\n",
    "\n",
    "# Print the evaluation metric after resampling\n",
    "print(f\"Accuracy after Feature Engineering and Resampling: {accuracy_resampled:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f8ac2",
   "metadata": {},
   "source": [
    "# 6. Model Interpretation:\n",
    "    a. Interpret the coefficients of the logistic regression model and discuss which features have the most influence on fraud detection.\n",
    "    b. Explain how the logistic regression model can be used for decision-making in identifying potential fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54759272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Interpret the coefficients of the logistic regression model and discuss which features have the most influence on fraud detection.\n",
    "\n",
    "    Here In logistic regression, coefficients represent a feature's influence on fraud detection. \n",
    "    Positive coefficients (e.g., 'transaction_amount') indicate increased fraud likelihood with higher values, while negative coefficients suggest the opposite. \n",
    "\n",
    "    Key influential features include 'transaction_amount' (higher amounts indicate risk), 'account_balances' (lower balances post-transaction signal risk), and 'transaction_types' (certain types pose higher fraud risks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99956453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Explain how the logistic regression model can be used for decision-making in identifying potential fraud.\n",
    "\n",
    "    The logistic regression model aids fraud identification by assigning probabilities to transactions. \n",
    "    If a transaction's probability exceeds a threshold, it's flagged as potential fraud. \n",
    "    Adjusting the threshold balances false positives and negatives. By analyzing these probabilities, businesses prioritize investigating transactions, enhancing fraud detection efficiency and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bcecc5",
   "metadata": {},
   "source": [
    "# 7. Model Comparison:\n",
    "    a. Compare the performance of the initial logistic regression model with the feature-engineered and balanced data model.\n",
    "    b. Discuss the advantages and limitations of each approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a50abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a. Compare the performance of the initial logistic regression model with the feature-engineered and balanced data model.\n",
    "\n",
    "a. Model Comparison:\n",
    "\n",
    "    Initial Logistic Regression Model:\n",
    "        Accuracy: [Initial Model Accuracy]\n",
    "        Advantages: Simple, easy to implement, provides a baseline understanding of the data.\n",
    "        Limitations: May not capture complex patterns, especially in imbalanced datasets.\n",
    "    \n",
    "    Logistic Regression Model with Feature-Engineered and Balanced Data:\n",
    "        Accuracy: [Feature-Engineered Model Accuracy]\n",
    "        Advantages: Utilizes advanced techniques for feature engineering and handles class imbalance, improving overall predictive power.\n",
    "        Limitations: Complexity might be higher, potentially leading to overfitting with insufficient data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdedd047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b. Discuss the advantages and limitations of each approach.\n",
    "Initial Logistic Regression Model:\n",
    "    Advantages:\n",
    "        Simple and interpretable, making it easy to understand the model's decisions.\n",
    "        Faster training and prediction times, suitable for large datasets.\n",
    "\n",
    "    Limitations:\n",
    "        Limited ability to capture intricate relationships in the data, especially in the case of imbalanced classes.\n",
    "        Might lead to biased predictions due to class imbalance.\n",
    "\n",
    "Logistic Regression Model with Feature-Engineered and Balanced Data:\n",
    "    Advantages:\n",
    "        Captures complex patterns through feature engineering, enhancing predictive accuracy.\n",
    "        Handles class imbalance, reducing the risk of biased predictions.\n",
    "    Limitations:\n",
    "        Increased complexity might require more data for robust training, guarding against overfitting.\n",
    "        Longer training times and higher computational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa7a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
